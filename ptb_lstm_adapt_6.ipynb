{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx, math\n",
    "import argparse, math\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet.ndarray as F\n",
    "from mxnet import gluon, autograd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu(0)\n"
     ]
    }
   ],
   "source": [
    "context = mx.gpu(0)\n",
    "#context = mx.cpu(0)\n",
    "print (context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 512    #help='size of word embeddings')\n",
    "nhid = 512      #help='number of hidden units per layer')\n",
    "nlayers = 1     #help='number of layers')\n",
    "lr = 0.1          #help='initial learning rate')\n",
    "clip = 0.25     #help='gradient clipping')\n",
    "epochs = 10     #help='upper epoch limit')\n",
    "batch_size = 128    #help='batch size')\n",
    "bptt = 20           #help='sequence length')\n",
    "dropout = 0.0      #help='dropout applied to layers (0 = no dropout)')\n",
    "#tied = 'store_true'      #help='tie the word embedding and softmax weights')\n",
    "log_interval = 200    #help='report interval')\n",
    "save = 'model.params'      #help='path to save the final model')\n",
    "gctype = 'none'        #help='type of gradient compression to use, \\ takes `2bit` or `none` for now.')\n",
    "gcthreshold = 0.5       #help='threshold for 2bit gradient compression')\n",
    "#mode = 'lstm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8.train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "input = data['input']\n",
    "label = data['label']\n",
    "input = mx.nd.array(input)\n",
    "label = mx.nd.array(label)\n",
    "\n",
    "vocab = len(data['worddic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44369\n"
     ]
    }
   ],
   "source": [
    "print (vocab)\n",
    "ntokens = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptivesoftmax(gluon.Block):\n",
    "    def __init__(self, input_size, cutoff, reduce_factor=4):\n",
    "        # input_size refers to the hidden_size from LSTM/RNN\n",
    "        super(Adaptivesoftmax, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.cutoff = cutoff\n",
    "        self.output_size = cutoff[0] + len(cutoff) - 1\n",
    "\n",
    "        self.head = nn.Dense(units=self.output_size, in_units=input_size, flatten=False)\n",
    "        self.tail = nn.Sequential()\n",
    "        \n",
    "        for i in range(len(cutoff) - 1):\n",
    "            if reduce_factor == 1:\n",
    "                seq = nn.Dense(units=(cutoff[i + 1] - cutoff[i]), in_units=input_size, flatten=False)\n",
    "\n",
    "            else:\n",
    "                seq = nn.Sequential()\n",
    "                seq.add(nn.Dense(units=(input_size // reduce_factor ** i), \n",
    "                                 in_units=input_size, flatten=False))\n",
    "                seq.add(nn.Dense(units=(cutoff[i + 1] - cutoff[i]), \n",
    "                                 in_units=(input_size // reduce_factor ** i), flatten=False))\n",
    "\n",
    "            self.tail.add(seq)\n",
    "        \n",
    "    def set_target(self, target):\n",
    "        #this function helps to select the data for different clusters\n",
    "        self.id = []\n",
    "        target = target.asnumpy()\n",
    "\n",
    "        for i in range(len(self.cutoff)):\n",
    "            if i < (len(self.cutoff) - 1):\n",
    "                mask_1 = (target >= self.cutoff[i])\n",
    "                mask_2 = (target <= self.cutoff[i + 1])\n",
    "                mask = mask_1 * mask_2\n",
    "            else:\n",
    "                mask = (target < self.cutoff[0])\n",
    "            \n",
    "            mask = mask.reshape((mask.shape[1],mask.shape[0]))\n",
    "            if True in mask:\n",
    "                self.id.append(mask[0])\n",
    "\n",
    "            else:\n",
    "                self.id.append(None)\n",
    "        \n",
    "                       \n",
    "    def forward(self, input, target):\n",
    "        #this part is for training; it contains both forward and loss \n",
    "        #shape (1120,2001): (bptt*batch_size, cutoff[0] + len(cutoff) - 1)\n",
    "        output_head = self.head(input)\n",
    "        nnloss = 0\n",
    "        self.target = target\n",
    "        \n",
    "        if self.target is not None:\n",
    "            self.set_target(self.target)\n",
    "            \n",
    "        for i in range(len(self.id)-1):\n",
    "            if self.id[i] is not None:\n",
    "                id_select = np.array(self.id[i])\n",
    "                output_tail = self.tail[i](input[id_select])\n",
    "                prob_head = F.log_softmax(output_head[id_select])\n",
    "                split = prob_head[:,self.cutoff[0]+i].expand_dims(1)  \n",
    "                prob_tail = F.log_softmax(output_tail[id_select]) + split\n",
    "                \n",
    "                loss = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=True)\n",
    "                nnloss = nnloss + mx.nd.sum(loss(prob_tail, target[id_select]))\n",
    "                \n",
    "        if self.id[-1] is not None:\n",
    "                id_select = np.array(self.id[-1])\n",
    "                loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "                nnloss = nnloss + mx.nd.sum(loss(output_head[id_select], target[id_select]))            \n",
    "                \n",
    "        nnloss = nnloss / (len(target))    \n",
    "        return nnloss     \n",
    "    \n",
    "    def log_prob(self, input):  \n",
    "        #this part is for test and it does not requrie gradients\n",
    "        head_out = self.head(input)\n",
    "            \n",
    "        #target_size refers to bptt * batch_size\n",
    "        target_size = len(head_out[0])\n",
    "        prob = mx.nd.zeros((target_size, self.cutoff[-1]))\n",
    "            \n",
    "        lsm_head = mx.nd.log_softmax(head_out, axis=1)\n",
    "        prob[:, : self.cutoff[0]] = lsm_head[:, : self.cutoff[0]]\n",
    "        \n",
    "        for i in range(len(self.tail)):\n",
    "            split = lsm_head[:, self.cutoff[0] + i].unsqueeze(1)\n",
    "            tail_out = self.tail[i](input)\n",
    "            lsm_tail = mx.nd.log_softmax(tail_out, axis=1) + split\n",
    "            prob[:, self.cutoff[i] : self.cutoff[i + 1]] = lsm_tail\n",
    "        \n",
    "        return prob                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(gluon.Block):\n",
    "    def __init__(self, vocab_size, num_embed, num_hidden, num_layers, dropout=0.0,\n",
    "            adaptive_softmax=True, cutoff=[2000], **kwargs):\n",
    "        super(LanguageModel, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "\n",
    "            self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "\n",
    "        if adaptive_softmax:\n",
    "            self.linear = Adaptivesoftmax(num_hidden, [*cutoff, vocab_size + 1])\n",
    "        else:\n",
    "            self.linear = nn.Dense(units=vocab_size, in_units=num_hidden, flatten=False)\n",
    "            \n",
    "        self.adaptive_softmax = adaptive_softmax\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "    def forward(self, input, hidden, target=None, training=True):\n",
    "        #this part is for training\n",
    "        embed = self.encoder(input)\n",
    "        print (embed.shape)\n",
    "        embed = self.drop(embed)\n",
    "\n",
    "        output, hidden = self.rnn(embed, hidden)\n",
    "        print(output.shape)\n",
    "        output = self.drop(output)\n",
    "\n",
    "        if self.adaptive_softmax:\n",
    "            self.linear.set_target(target)\n",
    "\n",
    "        #(bptt*batch_size, hidden_size) \n",
    "        nnloss = self.linear(output.reshape(output.shape[0] * output.shape[1], output.shape[2]), target)\n",
    "\n",
    "        return nnloss, hidden\n",
    "         \n",
    "    def log_prob(self, input, hidden):\n",
    "        #this part is for test\n",
    "        embed = self.encoder(input)\n",
    "        output, hidden = self.rnn(embed, hidden)\n",
    "        prob = self.linear.log_prob(output.reshape(output.shape[0] * output.shape[1], output.shape[2]))\n",
    "\n",
    "        return prob, hidden            \n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(vocab_size=ntokens, num_embed=emsize, num_hidden=nhid, num_layers=nlayers, dropout=0.25,\n",
    "                       cutoff=[2000, 10000])\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 1e-5})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "languagemodel1_ (\n",
       "  Parameter languagemodel1_embedding0_weight (shape=(44369, 512), dtype=float32)\n",
       "  Parameter languagemodel1_lstm0_l0_i2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter languagemodel1_lstm0_l0_h2h_weight (shape=(2048, 512), dtype=<class 'numpy.float32'>)\n",
       "  Parameter languagemodel1_lstm0_l0_i2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter languagemodel1_lstm0_l0_h2h_bias (shape=(2048,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter dense5_weight (shape=(2002, 512), dtype=float32)\n",
       "  Parameter dense5_bias (shape=(2002,), dtype=float32)\n",
       "  Parameter dense6_weight (shape=(512, 512), dtype=float32)\n",
       "  Parameter dense6_bias (shape=(512,), dtype=float32)\n",
       "  Parameter dense7_weight (shape=(8000, 512), dtype=float32)\n",
       "  Parameter dense7_bias (shape=(8000,), dtype=float32)\n",
       "  Parameter dense8_weight (shape=(128, 512), dtype=float32)\n",
       "  Parameter dense8_bias (shape=(128,), dtype=float32)\n",
       "  Parameter dense9_weight (shape=(34370, 128), dtype=float32)\n",
       "  Parameter dense9_bias (shape=(34370,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def eval(data_source, target_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    for (data, target) in zip(data_source, target_source):\n",
    "        data = data.as_in_context(context).T\n",
    "        target = target.as_in_context(context).T.reshape((-1, 1))\n",
    "        prob, hidden = model.log_prob(data, hidden)\n",
    "        L = loss(prob, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "def train():\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "        i = 0\n",
    "        for (data, target) in zip(input, label):\n",
    "            data = data.as_in_context(context).T\n",
    "            target = target.as_in_context(context).T.reshape((-1, 1))\n",
    "            #print (data.shape, target.shape)\n",
    "            hidden = detach(hidden)\n",
    "            #print (F.max(target))\n",
    "            with autograd.record():\n",
    "                nnloss, hidden = model(data, hidden, target)\n",
    "                L = nnloss\n",
    "                L.backward()\n",
    "            \n",
    "            grads = [p.grad(context) for p in model.collect_params().values()]\n",
    "            #gluon.utils.clip_global_norm(grads, clip)\n",
    "\n",
    "            trainer.step(1, ignore_stale_grad=True)\n",
    "            total_L += L.asscalar()\n",
    "            \n",
    "            \n",
    "            i+=1\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, ppl %.2f'%(\n",
    "                    epoch, i, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
